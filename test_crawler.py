#!/usr/bin/env python3
"""
æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½
ç”¨æ–¼é©—è­‰çˆ¬èŸ²æ˜¯å¦èƒ½æ­£ç¢ºæŠ“å–è³‡æ–™
"""

import logging
import re
from datetime import date

from kobo_ical.crawler import KoboCrawler
from kobo_ical.config import Settings

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_crawler():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ Kobo 99 æ›¸å–®çˆ¬èŸ²...")
    print("=" * 60)
    
    settings = Settings()
    
    # æ¸¬è©¦ç•¶å‰é€±æ¬¡
    with KoboCrawler(settings) as crawler:
        year, week = crawler.get_current_week_info()
        print(f"ğŸ“… ç•¶å‰å¹´ä»½ï¼š{year}ï¼Œé€±æ•¸ï¼š{week}")
        print(f"ğŸ“… ç•¶å‰æ—¥æœŸï¼š{date.today()}")
        
        # æ¸¬è©¦å¤šå€‹é€±æ¬¡ï¼ˆç•¶å‰é€±å¾€å‰æ¨2é€±ï¼Œå¾€å¾Œæ¨2é€±ï¼‰
        start_week = max(1, week - 2)
        start_year = year
        if start_week <= 2:
            start_year -= 1
            start_week = 52 + start_week - 2
        
        end_week = min(52, week + 2)
        end_year = year
        if end_week >= 51:
            end_year += 1
            end_week = end_week - 52
        
        print(f"\nğŸ” æ¸¬è©¦ç¯„åœï¼š{start_year}å¹´ç¬¬{start_week}é€± åˆ° {end_year}å¹´ç¬¬{end_week}é€±")
        
        # æ¸¬è©¦ç”Ÿæˆ URL
        urls = crawler.generate_weekly_urls(start_year, start_week, end_year, end_week)
        print(f"\nğŸ”— å°‡æ¸¬è©¦ä»¥ä¸‹ {len(urls)} å€‹ URLï¼š")
        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url}")
        
        # å˜—è©¦æŠ“å–å¤šå€‹é€±æ¬¡
        print(f"\n{'=' * 60}")
        print("é–‹å§‹æŠ“å–æ›¸å–®...")
        print("=" * 60)
        
        all_books = []
        for test_url in urls[:3]:  # å…ˆæ¸¬è©¦å‰3å€‹ URL
            match = re.search(r'weekly-dd99-(\d{4})-w(\d+)', test_url)
            if match:
                test_year = int(match.group(1))
                test_week = int(match.group(2))
                print(f"\nğŸ“– æ¸¬è©¦ {test_year}å¹´ç¬¬{test_week}é€±...")
                try:
                    books = crawler.crawl_weekly_books(test_year, test_week, test_year, test_week)
                    if books:
                        print(f"  âœ… æ‰¾åˆ° {len(books)} æœ¬æ›¸ç±")
                        all_books.extend(books)
                        for book in books[:3]:  # åªé¡¯ç¤ºå‰3æœ¬
                            print(f"    - {book.title} ({book.date})")
                        if len(books) > 3:
                            print(f"    ... é‚„æœ‰ {len(books) - 3} æœ¬æ›¸")
                    else:
                        print(f"  âš ï¸  æœªæ‰¾åˆ°æ›¸ç±è³‡æ–™")
                except Exception as e:
                    print(f"  âŒ éŒ¯èª¤ï¼š{e}")
                    import traceback
                    traceback.print_exc()
        
        print(f"\n{'=' * 60}")
        print(f"ğŸ“Š ç¸½è¨ˆæŠ“å–çµæœï¼š{len(all_books)} æœ¬æ›¸ç±")
        print("=" * 60)
        
        if all_books:
            print("\nâœ… æˆåŠŸæŠ“å–çš„æ›¸ç±åˆ—è¡¨ï¼š")
            for i, book in enumerate(all_books, 1):
                print(f"\n{i}. {book.title}")
                print(f"   æ—¥æœŸï¼š{book.date}")
                print(f"   æ›¸ç±é€£çµï¼š{book.book_url}")
                print(f"   ä¾†æºæ–‡ç« ï¼š{book.article_url}")
        else:
            print("\nâš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ›¸ç±è³‡æ–™")
            print("\nå¯èƒ½åŸå› ï¼š")
            print("1. è©²é€±æ¬¡å°šæœªç™¼å¸ƒæ›¸å–®")
            print("2. ç¶²é çµæ§‹èˆ‡é æœŸä¸åŒ")
            print("3. Cloudflare ä¿è­·å°è‡´ç„¡æ³•è¨ªå•")
            print("4. é¸æ“‡å™¨éœ€è¦èª¿æ•´")
            print("\nå»ºè­°ï¼š")
            print("- æª¢æŸ¥ URL æ˜¯å¦å¯æ­£å¸¸è¨ªå•")
            print("- æŸ¥çœ‹æ—¥èªŒè¼¸å‡ºä»¥äº†è§£è©³ç´°éŒ¯èª¤")
            print("- å¯èƒ½éœ€è¦èª¿æ•´ crawler.py ä¸­çš„ HTML é¸æ“‡å™¨")

if __name__ == "__main__":
    test_crawler()

